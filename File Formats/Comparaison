Questions à se poser quand on veut choisir le format adéquat:

1. Quels outils de traitement va-t-on utiliser ?
	- Il faut vérifier si l'outil supporte le format visé

2. Est-ce que la structure de la data va changer durant le temps ?
	- Bien choisir le format qui permet le changement des fields de la data tout en gardant le meme code de traitement, et ainsi pouvoir traiter données nouvelles et historiques de la même façon.

3. Est ce que la divisibilité des fichiers est importante
	- CSV est divisible / XML ne l'est pas
	- Il faut bien choisir le format pour tirer profit du traitement distribué de l'outil (Hadoop par exemple).

4. Est ce que la compression par bloc est importante
	- Si le format de fichier ne supporte pas la compression,  et si le fichier est compressé, il devient indivisible.

5. Est-ce que les fichiers sont larges
	- Si les fichiers sont de taille inférieure à celle d'un block HDFS, la compression et la divisibilité ne sont pas très importantes.
	- Puisque les gros fichiers consomment beaucoup d'espace dans HDFS, il peut être fructueux d'utiliser la compression (Snappy ou LZO par exemple ...)

6. Qu'est ce qui compte le plus ? Les performances de traitement ou de requête ?
	- Trois types de performances à considérer:
		* Performances d'écriture
		* Performances de lecture partielle
		* Performances de lecture totale

	- Un format orienté colonne et compressible comme parquet ou orc est optimal pour les lectures aux dépens de l'écriture.
	- Par contre, le format CSV est rapide en ce qui concerne l'écriture mais lent en terme de lecture (row oriented).
	- Souvent on stocke nos données sous plusieurs formats adéquats pour chaque cas d'utilisation


Description des différents formats

1. Formats à éviter
	- Documents JSON et XML: single document per file => indivisibles

2. Text/CSV
	- Souvent utilisés pour échanger des données entre Hadoop et des systèmes externes
	- Lisibles est parsables
	- Utiles quand on veut faire un dump de hadoop vers une bd analytique
	- Par contre, ils ne supportent pas la compression par block
	- Evolution de schéma limitée

3. JSON Records
	- Différents des documents JSON. Chaque ligne contient un enregistrement sous format JSON => fichier divisible
	- Métadata stockée avec les données => Evolution de schéma possible
	- Pas de compression par block

4. Avro
	- "Best multi-purpose storage format"
	- Metadata enregistrée avec les données
	- Compressible et divisible
	- Possibilité d'ajout/renommage/suppression d'attributs en définissant un nouveau schéma

5. Sequence Files
	- Stockage binaire de façon similaire à CSV
	- La seule option d'évolution du schéma consiste à ajouter de nouveaux champs
	- Compressibles
	- Utiles pour stocker les résultats intermédiaires entre jobs map-reduce

6. RC (Record Columnar) Files
	- Orientés colonne
	- Compressible et bénéficie d'avantages de performances de compression et de requêtes significatifs
	- Nécessite plus de mémoire et calcul pour l'écriture

7. ORC (Optimized RC files) Files
	- Compression meilleure que les RC files
	- Optimisé pour Hive

8. Parquet
	- Orientés colonne
	- Compressible et bénéficie d'avantages de performances de compression et de requêtes significatifs
	- Evolution de schéma limitée, les nouvelles colonnes ne peuvent être ajoutées qu'à la fin de la structure
	- Optimisé pour Cloudera/Impala


Synthèse
 	Si vous stockez des données intermédiaires entre les travaux MapReduce, les sequence files sont préférés. Si la performance de la requête est la plus importante, ORC (HortonWorks / Hive) ou Parquet (Cloudera / Impala) sont optimales - mais ces fichiers prendront plus longtemps à écrire. Avro est un bon choix si votre schéma va changer avec le temps, mais la performance de la requête sera plus lente que ORC ou Parquet. Les fichiers CSV sont excellents si vous voulez extraire des données de Hadoop pour charger en masse dans une base de données.